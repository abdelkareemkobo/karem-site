[
  {
    "objectID": "oss/opensource.html",
    "href": "oss/opensource.html",
    "title": " Open Source",
    "section": "",
    "text": "My open source work has been focused on developer tools and infrastructure. I‚Äôve contributed to projects such as fastai, Metaflow, Kubeflow, Jupyter, and Great Expectations, as well as many others. I list some of these below:"
  },
  {
    "objectID": "oss/opensource.html#fastai",
    "href": "oss/opensource.html#fastai",
    "title": " Open Source",
    "section": " fastai",
    "text": "fastai\nI maintain and contribute to a variety of fastai projects. Below are the projects I‚Äôve been very involved in:\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nfastpages \n\n\nAn easy to use blogging platform for Jupyter Notebooks. \n\n\nCreator\n\n\nBlog, Talk\n\n\n\n\nnbdev \n\n\nWrite, test, document, and distribute software packages and technical articles all in one place, your notebook. \n\n\nCore Contributor\n\n\nBlog, Talk\n\n\n\n\nfastcore \n\n\nA Python language extension for exploratory and literate programming. \n\n\nCore Contributor\n\n\nBlog\n\n\n\n\nghapi \n\n\nA Python client for the GitHub API \n\n\nCore Contributor\n\n\n Blog\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/opensource.html#metaflow",
    "href": "oss/opensource.html#metaflow",
    "title": " Open Source",
    "section": " Metaflow",
    "text": "Metaflow\nI created notebook cards: A tool that allows you to use notebooks to generate reports, visualizations and diagnostics in Metaflow production workflows. Blog"
  },
  {
    "objectID": "oss/opensource.html#kubeflow",
    "href": "oss/opensource.html#kubeflow",
    "title": " Open Source",
    "section": " Kubeflow",
    "text": "Kubeflow\nI‚Äôve worked on several projects related to Kubeflow, mainly around examples and documentation:\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nGitHub Issue Summarization\n\n\nAn end-to-end example of using Kubeflow to summarize GitHub Issues. Became one of the most popular tutorials of Kubeflow. \n\n\nAuthor\n\n\nInterview with Jeremy Lewi\n\n\n\n\nkubeflow/codei-intelligence\n\n\nVarious tutorials and applied examples of Kubeflow. \n\n\nCore Contributor\n\n\nTalk\n\n\n\n\nThe Kubeflow Blog\n\n\nI used fastpages to create the official Kubeflow blog. \n\n\nCore Contributor\n\n\nSite\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/opensource.html#jupyter",
    "href": "oss/opensource.html#jupyter",
    "title": " Open Source",
    "section": " Jupyter",
    "text": "Jupyter\nI created the Repo2Docker GitHub Action, which allows you to trigger repo2docker to build a Jupyter enabled Docker images from your GitHub repository. This Action allows you to pre-cache images for your own BinderHub cluster or for mybinder.org.\nThis project was accepted into the official JupyterHub GitHub org."
  },
  {
    "objectID": "oss/opensource.html#great-expectations",
    "href": "oss/opensource.html#great-expectations",
    "title": " Open Source",
    "section": " Great Expectations",
    "text": "Great Expectations\nI developed the Great Expectations GitHub Action that allows you to use Great Expectations in CI/CD Workflows. Blog."
  },
  {
    "objectID": "oss/opensource.html#other",
    "href": "oss/opensource.html#other",
    "title": " Open Source",
    "section": " Other",
    "text": "Other\nI worked as a staff machine learning engineer at GitHub from 2017 - 2022. I led or created the following open source projects that explored the intersection of machine learning, data and the developer workflow:\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nCode Search Net \n\n\nDatasets, tools, and benchmarks for representation learning of code. This was a big part of the inspiration for GitHub‚Äôs eventual work on CoPilot. \n\n\nLead\n\n\n Blog, Paper\n\n\n\n\nMachine Learning Ops\n\n\nA collection of resources on how to facilitate Machine Learning Ops with GitHub. This project explored integrations with a wide variety of data science tools with GitHub Actions. \n\n\nCreator\n\n\nBlog\n\n\n\n\nIssue Label Bot\n\n\nA GitHub App powered by machine learning that auto-labels issues. \n\n\nCreator\n\n\nBlog, Talk\n\n\n\n\nCovid19-dashboard \n\n\nA demonstration of how to use GitHub Actions, Jupyter Notebooks and fastpages to create interactive dashboards that update daily.\n\n\n\nCreator\n\n\nNews Article\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#the-time-i-spend-with-kobo",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#the-time-i-spend-with-kobo",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "The time I spend with kobo",
    "text": "The time I spend with kobo\nI spend on average 8 hours on my laptop ‚Äúkobo‚Äù doing a lot of programming, machine learning experiments and browsing many taps.\nI don‚Äôt use my phone a lot on average 1 or 2 hours, so i wake up and go to see my little friend and start journaling, wirte my ideas, perparing my todo list.\nI have been using it for 4 years now, and it was a nice period i love that it hasn‚Äôt broken till now, everything is working good even the battery can stand for the 2.5 hours in the battery life with a lot of memory and cpu usage\nYou can easily say that my laptop is now a part of me i can‚Äôt live without it any more my dairy, movies, work and college is depending on it"
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#who-is-kobo",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#who-is-kobo",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "Who is ‚ÄúKobo‚Äù!",
    "text": "Who is ‚ÄúKobo‚Äù!\n\n\n\nKobo\n\n\n\nSystem Specifications\nMy laptop is a Gfthin 95 core i7 9th gen, 16 GB RAM, 512GB SSD and GTX 1660 Ti, 120Hz screen"
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#msi-vs-lenovo",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#msi-vs-lenovo",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "MSI vs Lenovo",
    "text": "MSI vs Lenovo\n\nMSI with Linux\nAfter buying it, it wasn‚Äôt configured with Windows, but was another DOS version so I had to install a Windows version on it. After 1 week I get a lot of errors on it with the external monitor and the WiFi stops working or is not configured. I tried every Hindi video about this problem and gave it to specialists to try it, but in vain, the problem happened again and again.\nThe solution: It was my first year at a computer science college, so the geeky thing was to use Linux. After some search, I installed Ubuntu and then tried a lot of distros like Void Linux, but the one that I picked and still use is the amazing PopOS! which is based on Ubuntu but better than it!\nThe WiFi problem doesn‚Äôt appear again! And for Nvidia graphics, it‚Äôs solved in PopOS! which has a command to install some drivers. And everything works smoothly\n\n\nThe Cost of It!\nI bought it for the cost of 20,000 EGP, which at the time I bought it was $X, but now 20,000 EGP is worth $XX due to problems in my country.\nIts cost at that time is really high for my budget and family, but I insisted on buying it because I knew it would stay with me for years, so I wanted a good one.\n\n\nHow I Decided to Buy It!\nThe phrase ‚ÄúMachine learning needs an Nvidia GPU‚Äù was dancing in my mind, so my main focus was on a nice GPU card in a laptop with a low budget. After some search, I found that the best choices are MSI or Lenovo, but the Lenovo version cost about 4000 EGP more than the MSI.\n\nI tested the keyboard of the Lenovo, and every time I felt like I wanted to cut my fingers off\n\nIt wasn‚Äôt available online, so I had to travel 3 hours to the store that sells it. It was a nice experience to buy something I wanted instead of a random gift from my father."
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#i-am-not-a-gamer",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#i-am-not-a-gamer",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "I Am Not a Gamer!",
    "text": "I Am Not a Gamer!\nI didn‚Äôt use it for gaming because I am not a gamer, but I used it heavily in programming and some machine learning and basic computer vision models, and it works so nicely!"
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#kobo-for-ml",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#kobo-for-ml",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "Kobo for ML",
    "text": "Kobo for ML\n Machine learning is a wide area, and you can use the GPU for 3 things:\n\nTrain a model from scratch\nFine-tune a model\nEvaluate a model (inference)\n\n\nTrain a model from scratch was an ideal task for such a GPU, but I was doing it for multiple computer vision classification tasks, learning new architectures, trying to build‚Ä¶etc. For ML you don‚Äôt always need a GPU but for deep learning, which is a subset of ML that uses more connected layers (which means more computing power and memory needs), you do.\nFine-tune a model is the case I used the most - I download a trained model and try to make it work better on my data. What you need here is enough GPU RAM - in my case it‚Äôs a 6GB card. This worked fine with computer vision algorithms and classic NLP models with less than 1 billion parameters. There‚Äôs no chance to try the LLAMAS models on such a card!\nEvaluate a model means to just load the model and not fine-tune it.\n\nI was fine with all the ML tasks I tried to do, unless I opened the door to large language models like the GPT family (ChatGPT for example). These models require a lot of memory and need a good graphics card like an RTX 30 or 40 series to test, and there‚Äôs no chance to train these models on any RTX card!\n\nWhy Not Just Use the Cloud!\nThere are two main solutions:\n\nKaggle\n\nKaggle has two GPU options, and I used it a lot, especially if the data was already hosted on Kaggle and was more than 10GB. Other than that, I downloaded a sample of the data and did my work on my own environment with CLI commands and VS Code configurations. This helped me a lot compared to just using the Kaggle editor, and it‚Äôs faster too! Sometimes the Kaggle kernel panics or just stops responding entirely!\n\nColab\n\nColab restarts after 4 hours and your work can get lost, and lots of annoying things like that happen a lot.\nYou have to pay for the GPU version. You get a number of hours to try it out, and it‚Äôs faster than my GTX 1660 by a good margin.\n\n\nBut I didn‚Äôt love these cloud solutions, and I found having my own local setup to be faster and more comfortable."
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#kobo-for-web-development",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#kobo-for-web-development",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "Kobo for Web Development",
    "text": "Kobo for Web Development\nI sometimes work on web projects (Django and JavaScript frameworks, especially AstroJS). I never had any issues building and testing web projects - everything worked nicely and efficiently."
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#the-battery-life",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#the-battery-life",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "The Battery Life",
    "text": "The Battery Life\nThe battery life can keep it working for 2.5 hours when the power is out if you switch to battery saver mode. I don‚Äôt think it could last more than 1 hour in normal mode! This is the laptop‚Äôs biggest issue. Sometimes it powers off even when fully charged if you do a lot of computation without plugging it in."
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#the-heat",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#the-heat",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "The Heat",
    "text": "The Heat\n I didn‚Äôt find heat to be a problem or even the fan noise in most of my work. But sometimes when running a deep learning model, the fan noise is a bit loud and it does get really hot. For normal coding and browsing though, there are no issues and the cooling system is fast. Overall I didn‚Äôt find heat to be a major problem."
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#the-keyboard",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#the-keyboard",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "The Keyboard",
    "text": "The Keyboard\nThe keyboard is very responsive and well configured with smooth clicks - no issues or sticky keys even though I have fat fingers!\nI‚Äôve been using this machine for 4 years and I write a lot. No broken keys have happened despite my clumsy fingers. I have a mechanical keyboard that I sometimes use, but I always miss the feel of the built-in keyboard. The red backlight is decent."
  },
  {
    "objectID": "blog/posts/mteb_encoding/my_little_dargon.html#finally",
    "href": "blog/posts/mteb_encoding/my_little_dargon.html#finally",
    "title": "My little Dragon ‚Äúkobo‚Äù",
    "section": "Finally!",
    "text": "Finally!\n I just wanted to say that I love ‚ÄúKobo‚Äù and I‚Äôve spent nice times with it - some really difficult and others really happy. It‚Äôs been a loyal friend and something to rely on."
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#introduction",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#introduction",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "Introduction",
    "text": "Introduction\n#defintion MTEB: MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB."
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#embedding-models",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#embedding-models",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "Embedding models",
    "text": "Embedding models\n\nText embedding models like GLove lack context awareness and ar=e thus commonly labeled as word embedding model. They consist of a layer mapping each input word to a vector often followed by an averaging layer to provide a final embedding invariant of input length.\nTransformers inject context awareness into language models via self-attention and form the foundation of most recent embedding models.\n\nBERT uses the transformer architecture and performs large-scale self-supervised pre-training. The resulting model can directly be used to produce text embeddings via an averaging operation alike Glove.\nSBERT be beneficial to perform additional fine-tuning of the transformer for competitive embedding performance.\nMost recent fine-tuned embedding models use a contrastive loss objective to perform supervised fine-tuned on positive and negative text pairs\n#critique Due to the large variety of available pretrained transformers,there is an at least equally large variety of potential text embedding models to be explored.This leads to confusion about which model provides practitioners with the best performance for their embedding use case."
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#the-problem",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#the-problem",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "The problem",
    "text": "The problem\n\nThe problem with the current evaluation regime of current text embedding models rarely covers the breadth of their possible use cases.\n\n#example SimCSE or SBERT solely evaluate on STS and classification tasks,leaving open questions about the transfer ability of the embedding models to search or clustering tasks.\n\nevaluating embedding methods on many tasks requires implementing multiple evaluation pipelines.\nimplementation details like preprocessing or hyperparameters may influence the results making it unclear whether performance improvements simply come from a favorable evaluation pipeline. This leads to the ‚Äúblind‚Äù application of these models to new use cases in industry or requires incremental work to reevaluate them on different tasks."
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#the-solution-with-this-benchmark",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#the-solution-with-this-benchmark",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "The solution with this benchmark",
    "text": "The solution with this benchmark\n\nMTEB consists of 58 datasets covering 112 languages from 8 embedding tasks:\n\n\nBitext mining\nclassification\nclustering\npair classification\nreranking, retrieval\nSTS\nsummarization.\n\n\nMTEB software is available open-source1 enabling evaluation of any embedding model by adding less than 10 lines of code.\nDatasets and the MTEB leaderboard are available on the Hugging Face Hub2 .\nWe evaluate over 30 models on MTEB with additional speed and memory benchmarking to provide a holistic view of the state of text embedding models. We cover both models available open-source as well as models accessible via APIs, such as the OpenAI Embeddings endpoint\nIt aims to sheds light on the weaknesses and strenghts of individual models,such as SimCSE‚Äôs(Gao et al., 2021b) low performance on clustering and retrieval despite its strong performance on STS."
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#the-mteb-desiderata",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#the-mteb-desiderata",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "The MTEB Desiderata",
    "text": "The MTEB Desiderata\nMETB is build on a set of desiderat.\n\nDiversity:\n\nit consists of 58 total datasets, 10 are multilingual, covering 112 different langauges.\nSentence-level and paragraph level datasets are included to contrast performance on short and long texts.\n\nSimplicity\n\nIt provides a simple API for plugging in any model that given a list of text can produce a vector for each list of texts can produce a vector for each list item with a consistent shape.\n\nExtensibility\n\nyou can add new datasets for existing tasks via a single file that specifies the task and a Huggingface dataset name where the data has been uploaded.\nNew tasks require implementing a task interface for loading the data and an evaluator for benchmarking\n\nReproduciblity\n\nThrough versioning at a dataset and software level,they make it easy to reproduce results in METP.\nJSON files corresponding to all results available in this paper have been made available together with the MTEB benchmark"
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#tasks-and-evaluation",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#tasks-and-evaluation",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "Tasks and Evaluation",
    "text": "Tasks and Evaluation\n#definition Bitext Mining\nInputs are two sets of sentences from two different languages. For each sentence in the first set, the best match in the second set needs to be found.\n\nThe matches are commonly translations.\nThe Provided model i used to embed each sentence and the closest pairs are found via cosine similarity\nF1 serves as the main metric for bitext mining.Accuracy, precision and recall are also computed. #definition Classification\nA train and test set are embedded with the provided model.\nThe train set embeddings are used to train a logistic regression classifier with 100 maximum iterations, which is scored on the test set.\nThe main metric is accuracy with average precision and f1 additionally provided. #definition Clustering Given a set of sentences or paragraphs, the goal is to group them into meaningful clusters.\nA mini-batch k-means model with batch size 32 and k equal to the number of different labels is trained on the embedded texts.\nThe model scored using V-measure - V-measure does not depend on the cluster label,thus the permutation of labels does not affect the score. #definition Classification A pair of text inputs is provided and a label needs to be assigned. Labels are typically binary variables denoting duplicate or paraphrase pairs.\nThe two texts are embedded and their distance is computed with various metrics (cosine similarity, dot product, euclidean distance, manhattan distance).\nUsing the best binary thresh- old accuracy, average precision, f1, precision and recall are computed.\nThe average precision score based on cosine similarity is the main metric. #definition Reranking : Inputs are a query and a list of relevant and irrelevant reference texts. The aim is to rank the results according to their relevance to the query.\nThe model is used to embed the references which are then compared to the query using cosine similarity.\nThe resulting ranking is scored for each query and averaged across all queries.\nMetrics are mean MRR@k and MAP with the latter being the main metric. #definition Retrieval Each dataset consists of a corpus, queries and a mapping for each query to relevant documents from the corpus.\nThe aim is to find these relevant documents.\nThe provided model is used to embed all queries and all corpus documents and similarity scores are computed using cosine similarity. After ranking the corpus documents for each query based on the scores, nDCG@k, MRR@k, MAP@k, precision@k and recall@k are computed for several values of k. nDCG@10 serves as the main metric.\nMTEB reuses datasets and evaluation from BEIR (Thakur et al., 2021). #definition Semantic Textual Similarity (STS) Given a sentence pair the aim is to determine their similarity. Labels are continuous scores with higher numbers indicating more similar sentences.\nThe provided model is used to embed the sentences and their similarity is computed using various distance metrics.\nDistances are benchmarked with ground truth similarities using Pearson and Spearman correlations.\nSpearman correlation based on cosine similarity serves as the main metric (Reimers et al.,2016). #definition Summarization A set of human-written and machine-generated summaries are provided. The aim is to score the machine summaries. The provided model is first used to embed all summaries. For each machine summary embedding, distances to all human summary embeddings are computed. The closest score (e.g.¬†highest cosine similarity) is kept and used as the model‚Äôs score of a single machine-generated summary. Pearson and Spearman correlations with ground truth human assessments of the machine-generated summaries are computed. Like for STS, Spearman correlation based on cosine similarity serves as the main metric\n\n\n#sidenote Non-Transformers : LASER (Heffernan et al.,2022) is the only context aware non-transformer model we benchmark, relying on an LSTM (Hochreiter and Schmidhuber, 1997) instead. Similar to LaBSE, the model trains on parallel data and focuses on bitext mining applications. ![[Pasted image 20231028124555.png]]"
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#analysis",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#analysis",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "Analysis",
    "text": "Analysis\n\nwe observe that there is considerable variability between tasks. No model claims the state-of-the-art in all seven English tasks.\nThere is even more variability in the results per dataset present in the appendix.\nFurther, there remains a large gap between self-supervised and supervised methods.\nSelf-supervised large language models have been able to close this gap in many natural language generation tasks (Chowd- hery et al., 2022).\nHowever, they appear to still require supervised fine-tuning for competitive em- bedding performance.\nWe find that performance strongly coorelates with model size, A majority of MTEB tasks are domainted by multi-billion parameter models.However, these come at a significant cost\nFor classification\n\nST5 models dominate the classification task across most datasets\nST5-XXL has the highest average performance, 3% ahead of the best non-ST5 model, OpenAI Ada Similarity\n\nClustering\n\nDespite being almost 50x smaller, the MPNet embedding model is on par with the ST5- XXL state-of-the-art on Clustering. This may be due to the large variety of datasets MPNet (and MiniLM) has been fine-tuned on.\nClustering requires coherent distances between a large number of embeddings.\nModels like SimCSE-sup or SGPTnli, which are only fine-tuned on a single dataset,NLI, may produce incoherent embeddings when encountering topics unseen during fine-tuning.\nRelatedly, we find that the query embeddings of SGPT-msmarco and the Ada Search endpoint are competitive with SGPT-nli and the Ada Similarity endpoint,respectively.\nWe refer to the public leaderboard5 for Ada Search results. This could be due to the MSMARCO dataset being significantly larger than NLI.\nThus, while the OpenAI docs recommend using the similarity embeddings for clustering use cases6 , the retrieval query embeddings may be the better choice in some cases.\n\nPair Classification\n\nGTR-XL and GTR-XXL have the strongest performance. Pair classification is closest to STS in its framing, yet models rank significantly differently on the two tasks. This highlights the importance of benchmarking on a diverse set of tasks to avoid blindly reusing a model for a different task.\n\nReranking\n\nMPNet and MiniLM models perform strongly on reranking tasks.\nOn SciDocsRR (Co-han et al., 2020a) they perform far better than big- ger models, which is likely due to parts of SciDocsRR being included in their training data.\nOur scale of experiments and that of model pre-training make controlling for data contamination challenging.\nThus, we ignore overlap of MTEB datasets with model training datasets in MTEB scores.\nAs long as enough datasets are averaged, we believe these effects to be insignificant.\n\nRetrieval\n\nSGPT-5.8B-msmarco is the best em- bedding model on the BEIR subset in MTEB as well as on the full BEIR benchmark (Thakur et al., 2021; Muennighoff, 2022).\nThe even larger 7.1B SGPT model making use of BLOOM (Scao et al., 2022) performs significantly weaker, which is likely due to the multilinguality of BLOOM.\nModels geared towards STS (SimCSE, ST5, SGPT- nli) perform badly on retrieval tasks.\nRetrieval tasks are unique in that there are two distinct types of texts: Queries and documents (‚Äúasymmetric‚Äù), while other tasks only have a single type of text (‚Äúsymmetric‚Äù).\nOn the QuoraRetrieval dataset, which has been shown to be largely symmetric (Muennighoff, 2022), the playing field is more even with SGPT-5.8B-nli outperforming SGPT- 5.8B-msmarco,\n\nSTS & Summarization\n\nRetrieval models (GTR, SGPT-msmarco) perform badly on STS, while ST5-XXL has the highest performance.\nThis highlights the bifurcation of the field into separate embedding models for retrieval (asymmetric) and similarity (symmetric) use cases (Muennighoff, 2022)."
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#efficiency",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#efficiency",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "Efficiency",
    "text": "Efficiency\nMaximum speed -&gt; Word Embedding models offer maximum speed with Glove taking the lead on both performance and speed, thus making the choice simple in this case\nMaximum performance -&gt; If latency is less important than performance Depending on the task at hand, GTR-XXL, ST5-XXL or SGPT-5.8B may be the right choice,\nSpeed and Peformance -&gt; The fine-tuned MPNet and MiniLM models lead the middle cluster making the choice easy.\n\n#todo you can check the gte architecture here it‚Äôs highly related to the MTEP [[tiny-gte_transformer_model]]"
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#conclusion",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#conclusion",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe found model performance on different tasks to vary strongly with no model claiming state-of-the-art on all tasks.\nOur studies on scaling behavior, model efficiency and multilinguality revealed various intricacies of models that should ease the decision-making process for future research or industry applications of text embeddings.\n\n\nThanks for reading. If you have any questions, feel free to comment down below or reach out to me on twitter @AbdelkareemElk1."
  },
  {
    "objectID": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#references",
    "href": "blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html#references",
    "title": "MTEB Massive Text Embedding Benchmark",
    "section": "References",
    "text": "References\n\nhttps://huggingface.co/blog/mteb\nhttps://arxiv.org/abs/2210.07316"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "üìù Papers",
    "section": "",
    "text": "These are a list of papers I‚Äôve given:\n\nExplainable Artificial Intelligence of Multi-Level Stacking Ensemble for Detection of Alzheimer‚Äôs Disease Based on Particle Swarm Optimization and the Sub-Scores of Cognitive Biomarkers"
  },
  {
    "objectID": "publish.html",
    "href": "publish.html",
    "title": "üìö Books",
    "section": "",
    "text": "Still cooking ‚Ä¶"
  },
  {
    "objectID": "publish.html#still-under-review",
    "href": "publish.html#still-under-review",
    "title": "üìö Books",
    "section": "",
    "text": "Still cooking ‚Ä¶"
  },
  {
    "objectID": "notes/video_editing.html",
    "href": "notes/video_editing.html",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c‚ÄìSNKiifuP8kEFh80wPu\nCMD + B -&gt; ‚ÄúBlade‚Äù CMD + SHIFT + [ or ] to cut to location\n\n\n\nHere is a circular camera filter with OBS, which might be easier than DVR.\nYou can crop like this\n\n\n\nYou can add pause recording as a hotkey in OBS",
    "crumbs": [
      "Video Editing"
    ]
  },
  {
    "objectID": "notes/video_editing.html#davinci-resolve",
    "href": "notes/video_editing.html#davinci-resolve",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c‚ÄìSNKiifuP8kEFh80wPu\nCMD + B -&gt; ‚ÄúBlade‚Äù CMD + SHIFT + [ or ] to cut to location\n\n\n\nHere is a circular camera filter with OBS, which might be easier than DVR.\nYou can crop like this\n\n\n\nYou can add pause recording as a hotkey in OBS",
    "crumbs": [
      "Video Editing"
    ]
  },
  {
    "objectID": "notes/video_editing.html#other-tools-to-look-into",
    "href": "notes/video_editing.html#other-tools-to-look-into",
    "title": "Video Editing",
    "section": "Other tools to look into",
    "text": "Other tools to look into\n\nDescript\nRunwayML\ncapcut - from Rajeev\nAdobe Premiere\nFrame - Video collaboration that you use for Upwork etc\nEpidemic Sound - Sound by mood (Sanyam)\nCayla - Artlist\nCayla - Premium Beat\n\nCayla recommmends 1080p / 24 FPS for Youtube",
    "crumbs": [
      "Video Editing"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html",
    "href": "notes/how-to-learn/index.html",
    "title": "How to learn",
    "section": "",
    "text": "I read the book Mindshift and it was unituitively so good that I decided to take this class. As a parent, I learned a bunch of things that I think will be beneficial to my children‚Äôs education.\nNotes from class Learning how to learn. These notes are for me and may not make sense for others.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html#focused-vs-diffused-mode",
    "href": "notes/how-to-learn/index.html#focused-vs-diffused-mode",
    "title": "How to learn",
    "section": "Focused vs Diffused Mode",
    "text": "Focused vs Diffused Mode\nYou can not access focus and diffused mode simultaneously.\nPeople have tried to access diffuse mode of thinking by bringing themselves to the point of sleep and waking up just as they fall asleep. For example, Salvador Dali - holding keys in your hand, and let the sound of keys falling the ground wake you up.\nExercise, going for a walk good way to access diffuse thinking. You must take notes right away b/c diffuse thoughts may evaporate very fast.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html#procrastination-memory-and-sleep",
    "href": "notes/how-to-learn/index.html#procrastination-memory-and-sleep",
    "title": "How to learn",
    "section": "Procrastination Memory and Sleep",
    "text": "Procrastination Memory and Sleep\nThey advocate the Pomodoro technique to combating procrastination. Its like HITT.\nPeriodic relaxation (every ~ 30 minutes) is important for accessing your diffuse mode. ‚ÄúIts important for the mortar to dry‚Äù.\nSpaced repetition (like Anki) is important for building memory. i\nGo over what you want to learn about right before you go to sleep, this will substantially improve the chances you will dream about it and form new connections about the subject.\nExercise can help create new neurons in your hippocampus (new neurons can be created there in adulthood) and help them survive longer.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html#writing-tips-diffuse-mode",
    "href": "notes/how-to-learn/index.html#writing-tips-diffuse-mode",
    "title": "How to learn",
    "section": "Writing Tips Diffuse Mode",
    "text": "Writing Tips Diffuse Mode\nDiffuse mode is very important for writing. Editing is like focus mode and creating ideas is diffuse mode. Some rules of thumb: - Do not outline, make a mind map - Do not edit while you are writing (this is really hard to do -&gt; turn off monitor and just write).https://writeordie.com - app that forces you to stay in diffuse mode. You really cannot look at the screen. - Repeating again, do not look at screen while you are writing! Only when editing.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html#chunking",
    "href": "notes/how-to-learn/index.html#chunking",
    "title": "How to learn",
    "section": "Chunking",
    "text": "Chunking\n‚ÄúTying your shoes‚Äù. Best chunks are subconscious. Spoken language is the best example of chunking. You have to practice to build chunks, you cannot just observe. You have to perform the task yourself.\nYou should scan a chapter before you read it: section headings, pictures, etc. This can help you build chunks.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html#illusions-of-competence",
    "href": "notes/how-to-learn/index.html#illusions-of-competence",
    "title": "How to learn",
    "section": "Illusions of competence",
    "text": "Illusions of competence\nRight after you read something, look away and repeat to yourself what you recall. You can also draw a concept map. The recall process actually improves memory.\nRecall is better than re-reading. Re-reading is effective when you let time pass so you get spaced repetition. You need to test yourself to make sure you are competent. Recall is a form of testing.\nRecall outside your place of study to strengthen your memory. This is because you can get queues from where you are studying.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html#deliberate-practice",
    "href": "notes/how-to-learn/index.html#deliberate-practice",
    "title": "How to learn",
    "section": "Deliberate Practice",
    "text": "Deliberate Practice\nFocus on the bits that you find difficult. Interleaving is important, meaning learning different subjects or even sections within one subject at once. Thomas S. Khun discovered that two types of people tend to make scientific breakthroughs: (1) young people (2) those who are trained in another discipline.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html#procrastination-and-memory",
    "href": "notes/how-to-learn/index.html#procrastination-and-memory",
    "title": "How to learn",
    "section": "Procrastination and Memory",
    "text": "Procrastination and Memory\nYou have already learned about the Pomodoro technique. There are other techniques.\nFocus on the process, not the product. Don‚Äôt focus on completing the homework, focus on the process that leads you to complete the homework. Process is the small chunks of time to chip away at the task. This is the idea behind the Pomodoro. Your only goal is to finish the Pomodoro, for example.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/how-to-learn/index.html#juggling-life-and-learning",
    "href": "notes/how-to-learn/index.html#juggling-life-and-learning",
    "title": "How to learn",
    "section": "Juggling Life and Learning",
    "text": "Juggling Life and Learning\nYou should make to-do list the night before for the next day and write it down. This will allow your subconscious to work on how it will conquer that task. Furthermore, writing it down will allow you to free it from working memory.\nPlan your quitting time is important.",
    "crumbs": [
      "How to learn"
    ]
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html",
    "href": "notes/web-scraping/browser-to-python.html",
    "title": "Browser requests to code",
    "section": "",
    "text": "I learned this from Zachary Blackwood‚Äôs 2022 NormConf Talk.",
    "crumbs": [
      "Web Scraping",
      "Browser requests to code"
    ]
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#example-get-a-list-of-subway-restaurants-with-python",
    "href": "notes/web-scraping/browser-to-python.html#example-get-a-list-of-subway-restaurants-with-python",
    "title": "Browser requests to code",
    "section": "Example: Get A List of Subway Restaurants With Python",
    "text": "Example: Get A List of Subway Restaurants With Python\n\nGo to https://www.subway.com/en-US/locator in Google Chrome\n\n\n\nOpen developer tools using Option + CMD + I\nGo the the network tab, and hit the clear button\n\n\n\nType in a zipcode and search. Look for a network request that seems like it is getting data, in this case GetLocations.ashx... looks super promising.\n\n\n\nRight click on that particular event and select Copy -&gt; Copy as Curl\n\n\n\nGo to curlconverter.com and paste the curl command there.\n\n\nEnjoy your python code that uses this otherwise undocumented API :)",
    "crumbs": [
      "Web Scraping",
      "Browser requests to code"
    ]
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#bonus-parse-the-response",
    "href": "notes/web-scraping/browser-to-python.html#bonus-parse-the-response",
    "title": "Browser requests to code",
    "section": "Bonus: Parse The Response",
    "text": "Bonus: Parse The Response\nYou can parse the response data in a hacky way.\n\n# run the code from curlconverter.com, which will give you a `response` object.\n\n&gt;&gt;&gt; import json\n... response_string = response.text\n... json_string = response_string[response_string.index(\"(\") +1:response_string.index('\"AdditionalData\":')-1]+'}'\n... parsed_string = json.loads(json_string)\n... stores = parsed_string['ResultData']\n\n&gt;&gt;&gt; stores\n[{'LocationId': {'StoreNumber': 21809, 'SatelliteNumber': 0},\n  'Address': {'Address1': '4888 NW Bethany Blvd',\n   'Address2': 'Suite K-1',\n   'Address3': 'Bethany Village Centre',\n   'City': 'Portland',\n   'StateProvCode': 'OR',\n   'PostalCode': '97229',\n   'CountryCode': 'US',\n   'CountryCode3': 'USA'},\n  'Geo': {'Latitude': 45.5548,\n   'Longitude': -122.8358,\n   'TimeZoneId': 'America/Los_Angeles',\n   'CurrentUtcOffset': 0},\n  'ListingNumber': 1,\n  'OrderingUrl': 'http://order.subway.com/Stores/Redirect.aspx?s=21809&sa=0&f=r&scc=US&spc=OR',\n  'CateringUrl': 'https://www.ezcater.com/catering/pvt/subway-portland-nw-bethany-blvd',\n  'ExtendedProperties': None},\n...",
    "crumbs": [
      "Web Scraping",
      "Browser requests to code"
    ]
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#when-to-use-this-approach",
    "href": "notes/web-scraping/browser-to-python.html#when-to-use-this-approach",
    "title": "Browser requests to code",
    "section": "When to use this approach",
    "text": "When to use this approach\nThis is great for adhoc things, but you probably want to use a headless browser and actually scrape the HTML if you want to do this in a repeatable way. But many times you want to do a one-off scrape, this isn‚Äôt so bad!",
    "crumbs": [
      "Web Scraping",
      "Browser requests to code"
    ]
  },
  {
    "objectID": "notes/web-scraping/transcribe-diarize.html",
    "href": "notes/web-scraping/transcribe-diarize.html",
    "title": "Transcribe & Diarize Videos",
    "section": "",
    "text": "I wanted to generate transcriptions of videos with speaker labels. Segmenting or labeling the speakers in audio like this is referred to as Diarization or Diarisation (wikipedia). Unfortunately, OpenAi‚Äôs Whisper doesn‚Äôt do diarization.",
    "crumbs": [
      "Web Scraping",
      "Transcribe & Diarize Videos"
    ]
  },
  {
    "objectID": "notes/web-scraping/transcribe-diarize.html#download-the-audio-file-with-yt-dlp.",
    "href": "notes/web-scraping/transcribe-diarize.html#download-the-audio-file-with-yt-dlp.",
    "title": "Transcribe & Diarize Videos",
    "section": "1. Download the audio file with yt-dlp.",
    "text": "1. Download the audio file with yt-dlp.\nThe -o \"audio.%(ext)s\" argument is used to name the output as audo.mp3. The %(ext)s is a placeholder for the file extension. The --extract-audio and --audio-format mp3 arguments are used to extract the audio from the video and convert it to mp3 format.\nyt-dlp --extract-audio --audio-format mp3 \\\n    -o \"audio.%(ext)s\" https://youtu.be/g_6nQBsE4pU\nThe above command will generate audio.mp3 in the current directory.",
    "crumbs": [
      "Web Scraping",
      "Transcribe & Diarize Videos"
    ]
  },
  {
    "objectID": "notes/web-scraping/transcribe-diarize.html#generate-the-transcript-with-diarization.",
    "href": "notes/web-scraping/transcribe-diarize.html#generate-the-transcript-with-diarization.",
    "title": "Transcribe & Diarize Videos",
    "section": "2. Generate the transcript with diarization.",
    "text": "2. Generate the transcript with diarization.\nThis is done with WhisperX. Make sure you carefully follow the instructions in the WhisperX repo corresponding to Speaker Diarization: you have to click on three Hugging Face repos and accept their terms & conditions.\nThe video I‚Äôm working with has 2 speakers, so that‚Äôs why I‚Äôm setting --min_speakers and --max_speakers equal to 2. The --hf_token argument is the Hugging Face token you get from following the instructions in the WhisperX repo.\nwhisperx audio.mp3 --model large-v2 --diarize \\\n    --min_speakers 2 --max_speakers 2 --hf_token &lt;your_hf_token&gt;\nThis will produce files with the following extensions audio.{srt, vtt, txt, tsv, json} in the current directory. You can limit the formats with --output_format and write these files to a different directory with --output_dir. The .json file contains the most detailed information about the diarization, with world-level predictions, whereas the .vtt and .srt files will contain a more human-readable transcript with speaker labels. I suggest looking at these files to see which one suits your needs.\nIf looking at the .json file, I recommend using jq with a command like this to see the first row of the segments array in that file:\njq '.segments[0]' audio.json",
    "crumbs": [
      "Web Scraping",
      "Transcribe & Diarize Videos"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "kareem's Blog",
    "section": "",
    "text": "This blog is where i struggle to learn about programming, deep learning and AGI.\nCurrently I‚Äôm Learning NLP, Generative AI and 3D vision to build charming language models that able to talk, hear and see, made me some coffee"
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "kareem's Blog",
    "section": "üíº Get In Touch",
    "text": "üíº Get In Touch\nDo you need help operationalizing ML, Recommendition systems or large language models?\nI‚Äôm open to consulting work and other forms of advisory. Email me at kareem01095134688@gmail.com if you‚Äôd like to chat!"
  },
  {
    "objectID": "index.html#feed",
    "href": "index.html#feed",
    "title": "kareem's Blog",
    "section": "üìÆ Feed",
    "text": "üìÆ Feed\nA curated collection of blog posts and shorter form notes.\n\n\n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n1/1/24\n\n\nMy little Dragon ‚Äúkobo‚Äù\n\n\n\n\n12/7/23\n\n\nAfter one year of using Huawei Mate 11 without google services\n\n\n\n\n12/6/23\n\n\nHuawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!\n\n\n\n\n10/28/23\n\n\nMTEB Massive Text Embedding Benchmark\n\n\n\n\n10/21/23\n\n\ntiny-gte ‚ÄúTiny, yet powerful, it is small in size but packs a lot of power.‚Äù\n\n\n\n\n7/18/23\n\n\nwhy i am blogging\n\n\n\n\n7/18/23\n\n\nhavrard CS197 AI research experiences\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#subscribe",
    "href": "index.html#subscribe",
    "title": "kareem's Blog",
    "section": "üì¨ Subscribe",
    "text": "üì¨ Subscribe\nSubscribe via  RSS."
  },
  {
    "objectID": "blog/feed.html",
    "href": "blog/feed.html",
    "title": "Kareem‚Äôs Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMy little Dragon ‚Äúkobo‚Äù\n\n\n\n\n\nA review about my MSI Gaming Laptop as a machine learning student and also a web developer! and want to say thanks for this amazing friend who leaved with me for 4 years.\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAfter one year of using Huawei Mate 11 without google services\n\n\n\n\n\nSharing my experience with using Huawei Mate product line for one year without huawei services, a detailed student review. describe my experience with, noteshelf,concepts, infinte painter, Flexcil and Gbox\n\n\n\n\n\nDec 7, 2023\n\n\nkareem\n\n\n\n\n\n\n\n\n\n\n\n\nHuawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!\n\n\n\n\n\nReview of the Huawei Freebuds 5i after 2 months of use. Huawei Freebuds is latest pair of noise-cancelling earbuds and share a lot in common with their stablemates the FreeBuds pro 2\n\n\n\n\n\nDec 6, 2023\n\n\nkareem\n\n\n\n\n\n\n\n\n\n\n\n\nMTEB Massive Text Embedding Benchmark\n\n\n\n\n\nMTEB Benchmark aims to provide clarity on how models perform on a variety of embedding tasks and thus serves as the gateway to finding universal text embedding applicable to a variety of tasks.\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntiny-gte ‚ÄúTiny, yet powerful, it is small in size but packs a lot of power.‚Äù\n\n\n\n\n\nlet‚Äôs explore this applications with txtai and other workflow with some notes and future directions\n\n\n\n\n\nOct 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwhy i am blogging\n\n\n\n\n\n\nlife\n\n\nblogging\n\n\npublish\n\n\n\nwhy i am blogging and should you also blog? Why you (yes, you) should blog blogging with astro js as a machine learning engineer\n\n\n\n\n\nJul 18, 2023\n\n\nKareem Elkhateb\n\n\n\n\n\n\n\n\n\n\n\n\nhavrard CS197 AI research experiences\n\n\n\n\n\nEmbark on a transformative journey into the world of scientific research, particularly deep learning, with our comprehensive 21-lecture course. Delve into a wealth of experiences and crucial insights delivered through quick, digestible lectures, designed for enthusiasts and beginners alike. Complete the course in just one or two days, exploring topics ranging from AI language models to advanced techniques in research paper analysis.\n\n\n\n\n\nJul 18, 2023\n\n\nkareem\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/life_style/why_I_am_blogging.html#why-i-am-blogging",
    "href": "blog/posts/life_style/why_I_am_blogging.html#why-i-am-blogging",
    "title": "why i am blogging",
    "section": "Why I am blogging",
    "text": "Why I am blogging\nHere is the revised version of your blog with corrected grammar:\n\nI am blogging to establish a presence on the internet and hope to collaborate with people who are interested in what I write or think about (reducing the search space by reversing the process).\nIt serves as a resume and blueprint for my existence in this world.\nMy goal is to help others by providing valuable content that makes their lives easier and better.\nOrganizing my knowledge and recapping topics I‚Äôve forgotten helps me experience ‚Äúaha‚Äù moments! üí°\n\nIt‚Äôs a test of whether I truly understand something or not.\n\nI don‚Äôt like social media posts, Facebook, Twitter, or LinkedIn articles. I believe these platforms are not the best places for technical blogs, and their disadvantages outweigh the advantages.\nI value the freedom of speech. Some topics or thoughts may be censored on social media, but I want to express my opinions without restrictions from others whose opinions I may not respect.\nI want to discuss topics while I‚Äôm learning them because they‚Äôre still fresh in my mind. This will help anyone in a similar position as me in the future.\nWhen studying a topic and knowing that I can write about it on my blog, I find my mind more focused on details that would fit well in the blog. This makes the learning process more enjoyable, focused, and valuable.\nI want my posts and knowledge to be more organized!\n\nWho will search for a post written six years ago on Facebook or see your first tweet?\nThis problem is related to the feed of these social platforms."
  },
  {
    "objectID": "blog/posts/life_style/why_I_am_blogging.html#references",
    "href": "blog/posts/life_style/why_I_am_blogging.html#references",
    "title": "why i am blogging",
    "section": "References",
    "text": "References\n\nhttps://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045"
  },
  {
    "objectID": "blog/posts/courses/havrard CS197 AI research experiences.html",
    "href": "blog/posts/courses/havrard CS197 AI research experiences.html",
    "title": "havrard CS197 AI research experiences",
    "section": "",
    "text": "This course consists of 21 quick lectures that include valuable experiences and important tips for anyone interested in the field of scientific research, especially deep learning. The course can be completed in approximately one or two days. The title is not very precise, and the content of the lectures ranges from about 8 to 26 pages each. Some topics may not be directly related to the specifics of scientific research, but they are generally very helpful lectures for beginners in the field."
  },
  {
    "objectID": "blog/posts/courses/havrard CS197 AI research experiences.html#table-of-contents",
    "href": "blog/posts/courses/havrard CS197 AI research experiences.html#table-of-contents",
    "title": "havrard CS197 AI research experiences",
    "section": "",
    "text": "This course consists of 21 quick lectures that include valuable experiences and important tips for anyone interested in the field of scientific research, especially deep learning. The course can be completed in approximately one or two days. The title is not very precise, and the content of the lectures ranges from about 8 to 26 pages each. Some topics may not be directly related to the specifics of scientific research, but they are generally very helpful lectures for beginners in the field."
  },
  {
    "objectID": "blog/posts/courses/havrard CS197 AI research experiences.html#reviews",
    "href": "blog/posts/courses/havrard CS197 AI research experiences.html#reviews",
    "title": "havrard CS197 AI research experiences",
    "section": "Reviews",
    "text": "Reviews\nLecture 1: Exciting Advances with AI Language Models Content : Interact with language models like GPT-3‚Äôs text completion and use Codex‚Äôs code generation abilities feedback : ‚≠ê (1/5)\n\nLecture 2: The Zen of python Content : vscode,git,conad,linting and Debugging. feedback: feedback : ‚≠ê (1/5)\n\nLecture 3: Reading AI Research papers Content :\n\nConduct a literature search to identify papers relevant to a topic of interest\nDifference between Reading Wide and Reading deep and how to balance between them\nHow to use Google Scholar and paper with code feedback : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)\n\n\nLecture 4: In-Tune with Jazz Hands Content:\n\nquick intro into huggingface\nTokenization\nCausal language modeling (CLM) feedback : ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)\n\n\nLecture 5: Lightning McTorch Content :\n\nFine-tuning A vision Transformer\nIntro to pytorch lightning (Lightning)\nData Loading\nHow to Build a Neural net Module with lightning and how lightning modules work feedback : ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)\n\n\nLecture 6 & 7: Moonwalking with Pytorch Content :\n\nPytorch Exercises\nTensors\nAutograd and neural networks feedback : ‚≠ê (1/5)\n\n\nLecture 8 & 9: Experiment Organization Spakrs Joy Content :\n\nWeight and Biases\nHyperparameter Search\nHydra feedback : ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)\n\n\nLecture 10 & 11 : I Dreamed a Dream Content\n\nIdentifying Gaps in A Research Paper\n\nCLIP and CheXzero\n\nGenerating Ideas for Building a Research Paper\nIterating on your research ideas feedback : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)\n\n\nLecture 12 & 13 : Today Was a Fairytale\n\nhow to deconstruct the elements of a research paper and their sequence\nResulting template that you can use as a general example feedback : ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)\n\n\nLecture 14 & 15: Deep Learning on Cloud Nine didn‚Äôt complete it üôÉüôÉüôÉ\n\nLecture 16 & 17:Make your dreams come tuned Content\n\nhigh level use of Stable Diffusion using a Dreambooth template\nUse AWS to accelerate the training of Stable Diffusion models with GPUs\nHF Accelerator feedback : ‚≠ê‚≠ê (2/5)\n\n\nLecture 18 : Research Productivity Power-Ups Content\n\nHow update meetings and working sessions\norganizing your efforts on a project\nwhat is technical dept and examples on it feedback : ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)\n\n\nLecture 19 :The AI Ninja Content\n\nHow to make Steady Progress\nSome Research Skills\nDiscussion Questions feedback : ‚≠ê‚≠ê (2/5) I found that Colah‚Äô blog content about research is better in the context and offers a great details\n\n\nLecture 20: Bejeweled ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê(5/5)\n\nhow to make a slides to improve your research talk\nAssertion Evidence Approach feedback : ‚≠ê‚≠ê (2/5) This is great related talk from MIT about this topic How to speak ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê(5/5)\n\n\nLecture 21 : Model Showdown Content\n\nStatistical Testing feedback : ‚≠ê‚≠ê ‚≠ê(3/5)"
  },
  {
    "objectID": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html",
    "href": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html",
    "title": "tiny-gte ‚ÄúTiny, yet powerful, it is small in size but packs a lot of power.‚Äù",
    "section": "",
    "text": "#definition : This is a¬†sentence-transformers¬†model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search. It is distilled from¬†thenlper/gte-small, with comparable (slightly worse) performance at around half the size."
  },
  {
    "objectID": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html#table-of-contents",
    "href": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html#table-of-contents",
    "title": "tiny-gte ‚ÄúTiny, yet powerful, it is small in size but packs a lot of power.‚Äù",
    "section": "",
    "text": "#definition : This is a¬†sentence-transformers¬†model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search. It is distilled from¬†thenlper/gte-small, with comparable (slightly worse) performance at around half the size."
  },
  {
    "objectID": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html#details",
    "href": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html#details",
    "title": "tiny-gte ‚ÄúTiny, yet powerful, it is small in size but packs a lot of power.‚Äù",
    "section": "Details",
    "text": "Details\n\nIt‚Äôs around ~45MB very small compared to other models like MiniLM-L6-V2 which is equal to ~80 MB\nEmbedding vector size 384d\nBERT based\nDistilied from thenlper/gte-small,"
  },
  {
    "objectID": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html#notice-about-using-small-size",
    "href": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html#notice-about-using-small-size",
    "title": "tiny-gte ‚ÄúTiny, yet powerful, it is small in size but packs a lot of power.‚Äù",
    "section": "Notice about using small size",
    "text": "Notice about using small size"
  },
  {
    "objectID": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html#use-cases",
    "href": "blog/posts/mteb_encoding/tiny-gte_transformer_model.html#use-cases",
    "title": "tiny-gte ‚ÄúTiny, yet powerful, it is small in size but packs a lot of power.‚Äù",
    "section": "Use Cases",
    "text": "Use Cases"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#intro",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#intro",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "Intro",
    "text": "Intro\nHi, In this fast blog i will talk about my review with Huawei Freebuds 5i, which is the first noise-cancelling earbuds i have tried. This is not an ad or even affiliate product, it‚Äôs my own review for fun and just seeking of knowledge."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#who-am-i",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#who-am-i",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "Who am i !",
    "text": "Who am i !\n\nI am Kareem, a college student interested in machine learning and web technology.\nI enjoy working deeply and seeking out quiet places, but I live in an area with some noisy disturbances in the mornings - people walking down the street, kids playing around the house, street vendors - which often interrupt my focus and break my state of flow.\nI wish there was a solution to block out all these sounds so I could live in an isolated place free of annoying noises! Using noise-cancelling headphones or heading into outer space would be ideal environments with no bothersome sounds around."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#how-i-bought-it",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#how-i-bought-it",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "how i bought it",
    "text": "how i bought it\n\nI bought it from amazon prime, and it cost me around 100$ or 3,200 pounds. It reached within two days."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#first-impressions",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#first-impressions",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "First impressions",
    "text": "First impressions\n\nUpon unboxing and using it for the first time, I was genuinely pleased with the initial track I played.\nThe sound quality was noticeably superior compared to my phone or any other earbuds I‚Äôve previously used. I then tested the noise-cancelling feature and was amazed when I couldn‚Äôt hear my younger brother calling me.\nThe street noises were effectively blocked out, with only a faint hum reaching my ears, which was quite tolerable. Initially, the earbud tips were a bit uncomfortable, but after switching to a smaller size, the discomfort was resolved. Everything seemed perfect, except for a slight pressure in my ears when using the noise-cancelling feature, particularly noticeable the following morning. Despite this, my ears seem to crave the earbuds and my skin seems to miss them when they‚Äôre not in use. This can be a bit of a downside, but it‚Äôs not always the case."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#is-noise-cancelling-actually-work",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#is-noise-cancelling-actually-work",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "Is noise cancelling actually work ?",
    "text": "Is noise cancelling actually work ?\nI‚Äôve used these earbuds in various settings:\n\nSubway: The noise-cancelling feature combined with my music created an immersive experience, effectively blocking out any other sounds and allowing me to enjoy my tunes in peace.\nCollege: Amidst the chatter and shouts of other students, I was able to retreat into my own world of sound.\nBus: The noise of the bus engine was successfully blocked out, but I could still hear the conversations of those sitting next to me. It wasn‚Äôt overly bothersome, but it didn‚Äôt provide complete isolation.\n\nIn summary, the noise-cancelling feature works well, but it doesn‚Äôt provide absolute silence. I can still hear some ambient sounds. One noticeable issue is that when I make calls using the earbuds, I can hear the other person clearly, but they often complain about the clarity of my voice. They say it sounds distant and unclear, sometimes even asking to call back later. This issue is particularly prevalent when I‚Äôm on the bus."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-modes-of-the-freebuds-and",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-modes-of-the-freebuds-and",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "the modes of the Freebuds and",
    "text": "the modes of the Freebuds and\nHere is a rephrased version of the key points about the Huawei Freebuds 5i‚Äôs features:\nNoise Cancellation Modes:\n\nNoise Cancelling Mode: Actively cancels out ambient noise. Uses more battery power.\nOff Mode: No active noise cancellation, but still provides some passive noise isolation. Less battery usage.\nAwareness Mode: Allows ambient sounds to be heard clearly. Useful for hearing announcements at the gym or people talking to you. However, my own voice sounds muted in this mode, so I have to remove the earbuds temporarily to hold conversations.\n\nSound Quality Presets:\n\nDefault: No sound enhancements applied.\nTreble Boost: Boosts treble frequencies. I haven‚Äôt used this.\nBass Boost: Emphasizes bass when listening to music like hip-hop.\nVoices: Optimizes sound for speech clarity. I use this for podcasts.\n\nConnection Priority Modes:\n\nBalance Mode: Balances audio quality and connection stability.\nSound Quality Priority: Prioritizes sound quality over connectivity. Uses more power which may cause occasional lag.\n\nI have not noticed any discernible difference between these two modes, so I just use the Balance mode."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-gestures",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-gestures",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "The Gestures",
    "text": "The Gestures\nThe Huawei Freebuds 5i offer various gesture controls:\n\nDouble-tap, Triple-tap, Press & hold, Swipe\n\nI particularly enjoy the swipe gesture for volume control, as it works smoothly. The press and hold gesture is a bit slow, and the triple-tap gesture can be annoying since tapping your ear three times isn‚Äôt very comfortable.\nOccasionally, the Freebuds don‚Äôt recognize when I‚Äôve inserted the left or right earbud, and I continue listening with just one earbud without realizing the other isn‚Äôt connected. This doesn‚Äôt happen often, though."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-compatibility-with-other-devices",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-compatibility-with-other-devices",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "The compatibility with other devices",
    "text": "The compatibility with other devices\n\nI love how it connect with both the phone and tablet without any problems or conflict\nI‚Äôve been using the Freebuds with my Realme phone, MSI Linux laptop, and Huawei Mate Pad 11.\nThe connection with the Realme phone is seamless, regardless of whether the AI Life application is used or not.\nWhen it comes to the Huawei Mate Pad 11, there‚Äôs a feature that allows you to adjust settings directly from the Bluetooth menu. This is a functionality that isn‚Äôt available on standard Android devices without the AI Life app.\n\n\nLinux connection\nI‚Äôve connected the Freebuds to my Linux laptop using the ‚Äòbluetoothctl‚Äô command. However, I‚Äôve encountered an issue where I need to restart Bluetooth each time I want to establish a connection for it to work properly"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-case-and-overall-design",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-case-and-overall-design",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "The case and overall design",
    "text": "The case and overall design\n\nThe design of the Freebuds is truly appealing. It has a unique aesthetic that sets it apart from other earbuds, giving it a premium feel.\nIn terms of design, the FreeBuds 5i bear a resemblance to the 4i model, but they are 11% lighter and have shorter stems. They come with an IP54 rating, making them dust-tight and splash-resistant. The color options include a new ‚ÄúIsle Blue‚Äù, along with ‚ÄúNebula Black‚Äù and ‚ÄúCeramic White‚Äù. The package includes small, medium, and large silicone eartips, as well as a short USB-C cable for charging the case.\nI love the sound of closing it, it‚Äôs a loud sound but lovely :)"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-battery",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#the-battery",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "The battery",
    "text": "The battery\n\nThe battery is really nice; I haven‚Äôt felt that I am missing the need to recharge it or that it has gone off at any time.\n\n\nBattery capacity\n\nPer earbud: 55 mAh (min)*\nCharging case: 410 mAh (min)*\n\n\n\nPlaytime\n\nMusic playback on 1 charge: 6.0 hours (with ANC enabled)**\nMusic playback on 1 charge: 7.5 hours (with ANC disabled)**\nMusic playback with charging case: 18.5 hours (with ANC enabled)**\nMusic playback with charging case: 28 hours (with ANC disabled)**\n\n\n\nCharging Time\n\nAbout 60 minutes for the earbuds (in the charging case)***\nAbout 110 for charging case without earbuds (wired)***"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei freebuds 5i.html#references",
    "href": "blog/posts/products_reviews/Huawei freebuds 5i.html#references",
    "title": "Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!",
    "section": "References",
    "text": "References\n\nhttps://smarttech101.com/bluetoothctl-management-of-bluetooth-devices-in-linux/\nhttps://askubuntu.com/questions/1225896/huawei-freebuds-3-pairing-with-ubuntu-18-04"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei_mate_pad_11.html",
    "href": "blog/posts/products_reviews/Huawei_mate_pad_11.html",
    "title": "After one year of using Huawei Mate 11 without google services",
    "section": "",
    "text": "Here is an expanded version of your blog post with some additional details:"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei_mate_pad_11.html#table-of-contents",
    "href": "blog/posts/products_reviews/Huawei_mate_pad_11.html#table-of-contents",
    "title": "After one year of using Huawei Mate 11 without google services",
    "section": "",
    "text": "Here is an expanded version of your blog post with some additional details:"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei_mate_pad_11.html#why-i-bought-it",
    "href": "blog/posts/products_reviews/Huawei_mate_pad_11.html#why-i-bought-it",
    "title": "After one year of using Huawei Mate 11 without google services",
    "section": "Why I bought it",
    "text": "Why I bought it\nI purchased the Huawei tablet because I enjoy creative activities like drawing, taking notes, and reading books. I wished to have an iPad or premium tablet with a good stylus to fully explore my creative potential. My brother found an excellent deal on this Huawei tablet for around 60% off retail price and gifted it to me. I‚Äôm very thankful to him!"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei_mate_pad_11.html#pros",
    "href": "blog/posts/products_reviews/Huawei_mate_pad_11.html#pros",
    "title": "After one year of using Huawei Mate 11 without google services",
    "section": "Pros",
    "text": "Pros\n\nExcellent audio quality with quad speakers\nGorgeous 10.95-inch LCD display\n\n2560x1600 resolution\n120Hz refresh rate for smooth visuals\n\nFast charging capabilities\n\nLarge 7250 mAh battery\nSupports 22.5W fast wired charging\n\nComfortable lightweight design, easy to use for long periods\nSeamless integration with other Huawei devices like watches and earbuds\nResponsive stylus with good palm rejection"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei_mate_pad_11.html#cons",
    "href": "blog/posts/products_reviews/Huawei_mate_pad_11.html#cons",
    "title": "After one year of using Huawei Mate 11 without google services",
    "section": "Cons",
    "text": "Cons\n\nTablet gets hot with prolonged intensive use like drawing or taking notes. Quite annoying.\nBattery life is decent but not enough to last a full day with heavy usage\nFrequent ads in App Gallery and default music app are frustrating\n\nShould not see ads just trying to open App Gallery\nDefault music app tries to push online streaming service with more ads\n\nMany apps spam notifications asking to reopen them. Very irritating."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei_mate_pad_11.html#using-huawei-devices-with-linux",
    "href": "blog/posts/products_reviews/Huawei_mate_pad_11.html#using-huawei-devices-with-linux",
    "title": "After one year of using Huawei Mate 11 without google services",
    "section": "Using Huawei Devices with Linux",
    "text": "Using Huawei Devices with Linux\n\nFile transfers require a cable, no wireless sharing\nMust use third party apps like KDE Connect for notifications\nCan‚Äôt develop custom themes or scripts due to lack of developer tools\nSyncing to Linux with Syncthing is slow and buggy\nOverall poor integration with Linux compared to Android/Windows"
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei_mate_pad_11.html#issues-with-google-services-on-huawei",
    "href": "blog/posts/products_reviews/Huawei_mate_pad_11.html#issues-with-google-services-on-huawei",
    "title": "After one year of using Huawei Mate 11 without google services",
    "section": "Issues with Google Services on Huawei",
    "text": "Issues with Google Services on Huawei\n\nStylus Apps\n\nInfinite Painter drawing app has broken stylus support and no pressure sensitivity\nNoteShelf note taking app can‚Äôt sync properly with cloud drives\nNebo note taking app has subpar palm rejection and no Arabic language support\nFlexcil best ebook reader/annotator but lacks Google Drive integration\n\n\n\nGbox Solution\n\nProvides access to YouTube, Maps and other Google apps\nAvailable on App Gallery with native integration\nMinimal battery drain\nBut lacks support for many Google services like Podcasts\nBuggy syncing with Google Keep\nCan‚Äôt leverage Google Drive within other apps\n\n\n\nAPK Pure\n\nApps sometimes fail to open, just black screen\nToo many ads\nLacks reliability compared to Play Store\n\nOverall, while the Huawei tablet offers excellent hardware, the software experience is hampered by lack of Google services. This leads to janky third party solutions and poor integration with Linux systems. I still enjoy using the tablet but hope someday Huawei can properly resolve these issues."
  },
  {
    "objectID": "blog/posts/products_reviews/Huawei_mate_pad_11.html#references",
    "href": "blog/posts/products_reviews/Huawei_mate_pad_11.html#references",
    "title": "After one year of using Huawei Mate 11 without google services",
    "section": "references",
    "text": "references\n\nhttps://www.noteshelf.net/\nhttps://www.gboxlab.com/\nhttps://www.infinitestudio.art/painter.php"
  }
]